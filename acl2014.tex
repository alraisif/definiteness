\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}
\usepackage{comment}
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{acl2014}

\usepackage[linkcolor=blue]{hyperref}

\usepackage[round]{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{verbatim}

% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\jbmarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{J}}_{\textsc{B}}}}}}
\newcommand{\lslmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{LS}}_{\textsc{L}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\jb}[1]{\arkcomment{\jbmarker}{#1}{orange}}
\newcommand{\lsl}[1]{\arkcomment{\lslmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}

\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments
\newcommand{\ilbl}[1]{\mbox{\textbf{\textsc{#1}}}} % internal label
\newcommand{\llbl}[1]{\mbox{\textsc{#1}}} % leaf label

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
\newcommand{\draftnotice}[1]{{\it\small #1}\\} % for the draft version only
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\title{\draftnotice{Title, author list not final. Page limit: 5 + 2 bib} \ab{Learning Definiteness across Languages using Classifiers} \nss{maybe: A Classification and Classifier for Cross-linguistic Functions of Definiteness}}

\finalversion{\author{Archna Bhatia \ \ \  Chu-Cheng Lin \ \ \  Lori Levin \ \ \ \  Mandy Simons \\
\bf Fatima Talib Al-Raisi \ \ \  Laleh Roostapour \ \ \  Abhimanu Kumar  \\
\bf  Nathan Schneider \ \ \  Yulia Tsvetkov \ \ \  Jordan Bender \ \ \  Chris Dyer\\
Carnegie Mellon University\\
Pittsburgh, PA 15213}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Definiteness is a nonhomogeneous category across languages expressing various communicative functions (types of semantic, pragmatic and discourse- related information).   Languages differ in the grainsize and inventory of communicative functions  that are expressed as well as the syntactic means for expressing them.  There is a major difference between languages that express definiteness primarily through articles like English "the" and "a" and languages that express definiteness with word order and other syntactic constructions.   This paper presents an \yt{ a thorough linguistic analysis of } annotation scheme for the communicative functions of definiteness. \yt{Agree with Nathan -- the scheme was presented in the LREC paper. This paper presents the linguistic analysis and the classifier}  We applied the annotation scheme to a corpus of written and spoken English and built a classifier that uses lexical, morphological, and syntactic features to predict communicative functions.   This work has twofold benefits: linguistically, it discovers the grammaticalization of definiteness, some of which is obvious (e.g., articles) and some of which is not obvious (include an example from our findings) 
In addition, this work has the potential to improve language technologies such as machine translation, information extraction, entity tracking, and other semantic processing tasks. \yt{We work only on English, and I don't think we discover grammaticalization. I'd say that benefits are: (1) linguistically, it provides analysis and interpretation of the scheme, (2) computationally,  it presents a framework to predict \textit{semantic and pragmatic} properties of the communicative functions of definiteness which, unlike lexical and morphosyntactic features, are preserved in translation.  This framework can be used to discover definiteness grammaticalization across languages.}.  
\end{abstract}

\section{Introduction}

Languages display a vast range of variation with respect to definiteness. For example, while languages like English make use of definite and indefinite articles to distinguish between the discourse status of various entities ({\it the car} vs {\it a car}), many other languages, such as Czech, Hindi, Indonesian, Russian do not have definite articles or any articles at all. Even the languages that do have articles vary in terms of how these articles are used. For example, English uses only one definite article ‘the’ for entities irrespective of whether they have been mentioned previously in the discourse or are familiar to the hearer by some other means, but there are languages such as Hausa which have a specialized article for anaphoric definite entities, see example in Figure~\ref{fig:hausaanaphdef} (borrowed from \cite{lyons99}). 

\begin{figure}[htbp]
\vspace*{0.1cm}
\begin{footnotesize}
\begin{verbatim}
(1) Na  kawo  keke    din-ka 
    AUX bring bicycle the-2SGM 	
    'I brought your bicycle (previously mentioned).' 
\end{verbatim}
\end{footnotesize}
\label{fig:hausaanaphdef}
\end{figure}

There are other means to express (in-)definiteness, such as the use of affixes.   Arabic, for example, uses a definite prefix {\it al-} and indefinite suffix {\it -n}. \cite{chen04} shows that Chinese expresses (in-)definiteness through the use of various constructions, such as the existential construction for indefinite subjects and the {\it ba}- construction for definite direct objects. Demonstratives, personal pronouns and possessives (which are found in all languages) are other kinds of definite NPs. Besides this variation in the form of (in-)definite NPs within and across languages, there is also variability in what semantic, pragmatic, discourse- related functions (in-)definites express.   We will refer to these as communicative functions. 

The literature on definiteness suggests that it may express communicative functions corresponding to notions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality(\cite{birner94}, \cite{condoravdi92}, \cite{evans77}, \cite{evans80}, \cite{gundel88}, \cite{gundel93}, \cite{heim90}, \cite{kadmon87}, \cite{kadmon90}, \cite{lyons99}, \cite{prince92}, \cite{roberts03}, \cite{russell05} among many others).  Reductionist approaches to definiteness try to define a single communicative function of definteness.   For example, \cite{kadmon87}, \cite{evans80} have used the notion of semantic uniqueness to account for definite NPs, but pronouns do not display uniqueness effects. \cite{roberts03} proposes that the combination of uniqueness and a presupposition of familiarity can explain definiteness, but possessive definite descriptions ({\it John's daughter}) and the weak definites ({\it the son of Queen Juliana of the Netherlands}) cannot be explained using this combination.

%% The red phrases only have meaning to linguists.   We have to %% rephrase for computer scientists. 

In contrast to the reductionists, we …(I’ll finish editing this paragraph tomorrow.   It will say that we have a range of prototypical and radial communicative functions and we want to correlate them with lexical, syntactic, and morphological constructions.)

We take such linguistic observations to suggest that definiteness is not as homogeneous a category as many accounts have assumed. Instead, it should be seen as grammaticalization of many such semantic, pragmatic, discourse- related notions. Therefore, it seems relevant to identify what information different forms of (in-)definites express within a language as a goal in itself. Additionally, it is advantageous to identify the mapping between the form and meaning of (in-)definites for various NLP applications. For example, this mapping can be used to encode correct information during the automatic construction of knowledge bases or other information extraction or semantic processing tasks. Also, this mapping can be used for machine translation applications. Here the assumption is that even though the forms may change across languages, the meaning is maintained while translating from one language to another. It has been noted previously that machine translation systems face problems while translating from a language to another when the languages use different grammatical strategies. \cite{tsvetkov13} and \cite{stymne09} mention how translating from an article-language to an article-less language is problematic. If the mapping between different forms and the information they encode is known in both the source language and the target language, this information can be leveraged in improving machine translation across these languages.  

In \cref{sec:scheme}, we review the annotation scheme we have used to determine the mapping between the form and meaning it encodes with respect to (in-)definiteness. In \cref{sec:data}, we provide information about the English data we have annotated using this scheme. \cref{sec:modeling} is an overview of the classification framework we have used. In \cref{sec:eval}, we describe the measures we have used to evaluate the predictions against the gold standard for the held-out data. In \cref{sec:exp}, we describe our experiments and discuss the results. In \cref{sec:conclusion}, we summarize our findings and briefly discuss the future tasks we are undertaking or envision.    \yt{I think that detailed description of definiteness in linguistics in the beginning of this section deserves a section by itself; I'd split the Introduction section into Introduction (shorter, only what we do, motivation, contribution) and then Definiteness.} 

%\ab{Here we discuss about why we should study definiteness- linguistically a hard problem, also it has applications in machine translation. Discuss about grammaticalization as a general problem (differences across languages), then grammaticalization of definiteness across languages, some examples to show differences across languages.}
%\nss{we review the annotation scheme in \cref{sec:scheme}; etc.}

\section{Annotation scheme}\label{sec:scheme}
Based on the literature on definiteness, we compiled a list of semantic, pragmatic, and discourse- related features which are relevant for the form of (in-)definite NPs in English (and many other languages). \yt{ref to the LREC paper?} These features were put together in a hierarchical structure to ease the annotation effort for the annotators. At each step, the annotators make a decision and thus reduce the number of labels they have to consider before annotating an NP. This approach also increases the consistency in annotations due to the reduced effort on considering the options among which to select the correct label for the NP. The annotation scheme was revised through many iterations after attempts at annotating natural language data from different genres using the scheme. The current scheme is presented in Figure~\ref{fig:hierarchy}. 

The first distinction annotators make is whether the NP at hand is anaphoric (discourse old) or non anaphoric (discourse new). We do not annotate for uniqueness, hearer old/new distinction, specificity or genericity for the anaphoric NPs as those distinctions were deemed inconsequential for English anaphoric NPs\footnote{However, in future, we may include these features for anaphoric NPs as well if contrary evidence were found.}. \yt{For someone who is not familiar with the scheme, this would be very hard to understand without looking at the definiteness scheme. I think the scheme should be on this page, and description should always point to the place in the scheme. May be we can write all scheme-related terms using the same font as they appear in the scheme? } The anaphoric NPs’ category is further subdivided into basic anaphora and extended anaphora. The basic anaphora is used for entities which have been mentioned previously in the discourse. The extended anaphora is inspired by the bridging references \cite{clark77} had mentioned. These are used for entities which are not discourse old or even hearer old, but at the same time, they are not entirely new either.  See example in Figure~\ref{fig:bridging} (borrowed from \cite{poesio98}). 

\begin{figure}[htbp]
\vspace*{0.1cm}
\begin{footnotesize}
(2) I went to {\it a wedding} last weekend. {\it The bride} was a friend of mine. She baked {\it the cake} herself. 
\end{footnotesize}
\label{fig:bridging}
\end{figure}

Within the non anaphoric category, the annotators decide whether the NP denotes a unique NP, a non unique NP or a generic NP. For the nongeneric cases, further decisions are made based on whether the entity is hearer old or new, and if it is old, what kind of situation makes it old. For generics also, there are two categories depending on whether they appear with kind-level or individual- level predicates resulting in different properties of these NPs. Besides these, there are a few non-referential categories as well.

Besides the annotation categories, a few other decisions were also taken regarding annotations. For example, corresponding to the basic anaphora cases, the anaphoric links can be established with antecedent NPs in previous sentences as well as with NPs within the same sentence. Also to determine whether an NP gets a Same\_head or a Different\_head label, we refer to the closest antecedent even if it appears within the same sentence. Regarding annotations of nested NPs, we assume that these NPs are interpreted inside out. This helps determine the cases of Bridging\_RestrictiveModifier category. If an NP consists of a modifier that restricts its reference enough to make the referent identifiable to the addressees, the embedding NP is annotated with the Bridging\_RestrictiveModifier label. 


Figure~\ref{fig:excerpt} is an excerpt from Little Red Riding Hood demonstrating the above-mentioned decisions regarding annotations and illustrating some of the categories from our annotation scheme.


\begin{figure}[htbp]
…
…
…
\label{fig:excerpt}
\end{figure}






 
%\ab{A brief discussion of the Annotation scheme}
\nss{we should probably have a name for the scheme---something like ``Functions of Definiteness Across Languages (FDAL)''? and a name for the categories (I have been calling them semantic functions).}\ab{I actually like the name ``Functions of Definiteness Across Languages'' but am not sure how to incorporate it here.}
\nss{suggestion: lead with a discourse excerpt, ideally illustrating a few of the categories, a nested NP, and 
an anaphoric link across sentence boundaries. I can help format it.}

\nss{cite papers like this: \citet{bhatia14} (but probably anonymize this for review) or \citep{reiter10}}

\begin{figure*}\small
\begin{tabular}{p{.45\textwidth}p{.45\textwidth}}
\begin{itemize}
\item    \ilbl{Nonanaphora} $[-A,-B]$ \textbf{(00)}
  \begin{itemize}
  \item      \ilbl{Unique} $[+U]$ \textbf{(00)}
    \begin{itemize}
    \item        \ilbl{uniq\_Hearer\_old} $[-G,+O,+S]$ \textbf{(00)}
      \begin{itemize}
      \item          \llbl{uniq\_Physical\_copresence} $[+R]$ (00)
      \item          \llbl{uniq\_Larger\_situation} $[+R]$ (00)
      \item          \llbl{uniq\_predicative\_identity} $[+P]$ (00)
      \end{itemize}
    \item        \llbl{uniq\_Hearer\_new} $[-O]$
    \end{itemize}
   \item     \ilbl{Nonunique} $[-U]$
     \begin{itemize}
     \item       \ilbl{nonuniq\_Hearer\_old} $[+O]$
       \begin{itemize}
       \item         \llbl{nonuniq\_Physical\_copresence} $[-G,+R,+S]$
       \item         \llbl{nonuniq\_Larger\_situation} $[-G,+R,+S]$
       \item         \llbl{nonuniq\_predicative\_identity} $[+P]$
       \end{itemize}
     \item       \llbl{nonuniq\_Hearer\_new\_spec} $[-G,-O,+R,+S]$
     \item       \llbl{nonuniq\_nonspec} $[-G,-S]$
     \end{itemize}
   \item \ilbl{Generic} $[+G,-R]$
     \begin{itemize}
	   \item      \llbl{Generic\_kindLevel}
	   \item      \llbl{Generic\_individualLevel}
     \end{itemize}
  \end{itemize}
\end{itemize} &
\begin{itemize}
\item    \ilbl{Anaphora} $[+A]$
  \begin{itemize}
  \item      \ilbl{Basic} $[+O,-B]$
    \begin{itemize}
    \item        \llbl{Same\_head}
    \item        \llbl{Different\_head}
    \end{itemize}
  \item  \ilbl{Extended} $[+B]$
    \begin{itemize}
    \item        \llbl{Bridging\_nominal} $[-G,+R,+S]$
    \item        \llbl{Bridging\_event} $[+R,+S]$
    \item        \llbl{Bridging\_restrictiveModifier} $[-G,+S]$
    \item        \llbl{Bridging\_subtype\_instance} $[-G]$
    \item        \llbl{Bridging\_OtherContext} $[+O]$
    \end{itemize}
  \end{itemize}
\item \ilbl{Miscellaneous} $[-R]$
  \begin{itemize}
	\item \llbl{Pleonastic} $[-B,-P]$
	\item \llbl{Quantified}
	\item \llbl{Predicative\_equative\_role} $[-B,+P]$
	\item \llbl{Part\_of\_noncompositional\_mwe}
	\item \llbl{Measure\_Nonreferential}
	\item \llbl{Other\_Nonreferential}
  \end{itemize}
\end{itemize}
\end{tabular}
\caption{Taxonomy of definiteness functions, with number of occurrences in the training data \nss{TODO}. 
Internal (non-leaf) labels are in bold.\nss{added two: Generic and Miscellaneous, and also reversed Nonreferential to Referential. is that OK?}\ab{yes, that is fine}\nss{TODO: normalize capitalization}
$+$/$-$ values are shown for ternary attributes \uline{A}naphoric, \uline{B}ridging, \uline{G}eneric, Hearer-\uline{O}ld, \uline{P}redicative, \uline{R}eferential, \uline{S}pecific, and \uline{U}nique; 
these are inherited from supercategories, but otherwise default to $0$.\nss{nonuniq\_nonspec is $-$generic, right?} \ab{right}
Thus, for example, the full attribute specification for \llbl{uniq\_Physical\_copresence} is $[-A,-B,-G,+O,0P,+R,+S,+U]$.}
\label{fig:hierarchy}
\end{figure*}

\section{Data}\label{sec:data}

We annotated data from a few genres, namely TED talks, news articles, speech (Presidential inauguration speech) and stories in English. However, currently the majority of our annotated data is from the TED talks corpus. For future work, we will be annotating other genres more as well. We annotated a total of 1783 sentences \ab{that is all our data, some is not yet annotated, am working on it today, hopefully we will be able to use most of it, otherwise we will change the numbers here} (a total of 28574 words), which consist of 10476 NPs (i.e. the annotatable units). 

A total of 6 annotators contributed to the annotations. Two of the annotators are linguists while others are computer scientists working on languages and an undergraduate student of Linguistics. Two annotators annotated most of the data, the inter annotator agreement between them was very high with Cohen's Kappa score of 0.94 within the TED corpus and 0.91 for combined genres.  However the other 4 annotators also annotated some of the data, but one of the two annotators with most experience took these data as inputs and revised annotations according to their annotations. This two-step process of annotation was taken to increase the amount of data within a short period of time and also to achieve consistency within annotations. All annotators were trained using the annotation guidelines and example annotated texts. Then the annotators were asked to annotate some data, these annotations were discussed to reach at consensus. The annotations used for discussions to reach at consensus were not included while calculating the inter annotator agreement.    

\ab{description of data, IAA, other statistics- n(annotations), n(annotations/ category)
classifiers for English- baseline, accuracy etc, confusion matrix- classifier predictions and gold standard
may be a brief discussion of feature ablation study-  which features are most helpful at least in these languages corresponding to individual categories (a clue to grammaticalization)
Principle component analysis, clustering of features}

\section{Classification framework}\label{sec:modeling}

To model the relationship between the grammar of definiteness and its semantic functions in a data-driven fashion,
we work within the supervised framework of feature-rich discriminative classification, 
treating the functional categories from \cref{sec:scheme} as output labels $y$
and various lexical, morphological, and syntactic characteristics of the language as features of the input $x$.
Specifically, we learn a probabilistic log-linear model similar to multiclass logistic regression, 
but deviating in the following ways:
\begin{itemize}
  \item Logistic regression treats each output label (response) as atomic; 
  we decompose each into \emph{attributes} based on their linguistic definitions, 
  enabling commonalities between related labels to be recognized.
  Each weight in the model corresponds to a feature that mediates between 
  \emph{percepts} (characteristics of the input noun phrase) and attributes (characteristics of the label).
  \item Logistic regression assumes a prediction is either correct or incorrect.
  We incorporate a \emph{cost function} that gives partial credit during learning when a related label 
  is predicted, so the learned model will better match our evaluation measure.
  \item Logistic regression assumes the space of possible predictions matches 
  the space of labels observed in the training data; we allow more abstract labels to be predicted, 
  which can receive partial credit. The scoring scheme encourages the predictor to ``back off'' 
  to a coarser label if it is not sufficiently confident about a fine-grained label.
\end{itemize}
These decisions are aimed at attaining better predictive accuracy 
as well as feature weights that better describe the form--function interactions we are interested in recovering.

Our setup is formalized below, where we discuss the mathematical model and linguistically motivated features.

\subsection{Model}

At test time, we model the probability of semantic label $y$ 
conditional on a \nss{gold?} noun phrase $x$ as follows:
\begin{equation}
p_{\boldsymbol{\theta}}(y | x) = \log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')}}}
\end{equation}
where $\boldsymbol{\theta} \in \mathbb{R}^d$ is a vector of parameters (feature weights), and 
$\mathbf{f}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ is the feature function over input--label pairs.
The feature function is defined as follows:
\begin{equation}
\mathbf{f}(x,y) = \boldsymbol{\phi}(x) \times \tilde{\boldsymbol{\omega}}(y)
\end{equation}
where the percept function $\boldsymbol{\phi}: \mathcal{X} \rightarrow \mathbb{R}^c$ 
produces a vector of real-valued characteristics of the input, and  
the attribute function $\tilde{\boldsymbol{\omega}}: \mathcal{Y} \rightarrow \{0,1\}^a$
encodes characteristics of each label.
There is a feature for every percept--attribute pairing: so
$d = c \cdot a$ and $f_{(i-1)a+j}(x,y) = \phi_i(x)\tilde{\omega}_j(y), 1 \leq i \leq c, 1 \leq j \leq a$.
The contents of the percept and attribute functions are detailed in \cref{sec:attrs,sec:feats}.

For prediction, having learned weights $\hat{\boldsymbol{\theta}}$ we choose the $y$ that maximizes this probability:
\begin{equation}
\hat{y} \leftarrow \arg\max_{y' \in \mathcal{Y}} p_{\hat{\boldsymbol{\theta}}}(y | x)
\end{equation}

Training optimizes $\hat{\boldsymbol{\theta}}$ so as to maximize the $L_1$-regularized
\emph{softmax-margin} learning objective \citep{gimpel} over the training data $\mathcal{D}$:
\begin{align*}
\hat{\boldsymbol{\theta}} &= \argmax_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \mathcal{D}) \\
\begin{split}
L(\boldsymbol{\theta}, \mathcal{D}) &= -\lambda ||\boldsymbol{\theta}||_1 \\ 
+ \sum_{\langle x,y \rangle\in\mathcal{D}} &\log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\left(\boldsymbol{\theta}^{\top}\mathbf{f}(x,y') + \textit{cost}(y,y')\right)}}}
\end{split}
\end{align*}
The \emph{cost function} allows us to penalize some errors more than others during training, 
taking into account the linguistic functions of the labels.
It is zero for the gold label and nonnegative for the others.\nss{intuition}

This framework gives us several ways to design a classifier appropriate to the task: 
the attributes, the space of labels $\mathcal{Y}$ to consider, 
and the cost function, and of course, the features themselves.
With $\textit{cost}(y,y') = 0$, $\mathcal{Y} = \{\textit{gold labels in training}\}$, 
and $\tilde{\boldsymbol{\omega}}(y) = \textit{the identity of the label}$, 
this reduces to standard logistic regression.

\subsection{Features}\label{sec:feats}
\begin{description}
	\item[Features of words of interest] The text is parsed with a dependency parser. In an NP parse there are certain words that we are mostly interested in. They include its \emph{head},
    \emph{dependents}, and \emph{outer heads} (which are words that the heads depend on). We are also interested in the \emph{attached verb}, which is the first verb one encounters when going upward the dependency path from the head. We extract their part-of-speech,
    lemmas, and grammatical information (of which we only collect plurality right now). Since there may be multiple dependents,
    we have special features for the first and the last one. Moreover, to better capture tense, aspect and modality, we collect the attached verb's \emph{aux} words. We also make note of \emph{neg} if it is attached to the verb.
    Of these words we collect their lemmas, POS's and dependency relation. Since there may be many dependents, we also specifically mark the first and the last word.
    
    \item[Parse structure related features] These features include length of the upward path from the head to the root, and to the attached verb. We also have features for the number of dependents, and the number of dependency relations that link non-neighbors.
    \item[Position related features] Including the length of the NP, NP position in the sentence (upper half or lower half), relative position of the verb to the head (left or right). We also collect information of the words of interest' left and right neighbors.
    \item[Other NPs] These include features in annotated NPs such as \emph{immediate parent}, which is the smallest NP that fully covers this NP, and similarly \emph{immediate child}. We also have \emph{immediate precedent} and \emph{immediate successor}. Using a coreference resolution program, we identify mentions, and extract their features as well.
    
\end{description}

%\nss{TODO}

\subsection{Attributes}\label{sec:attrs}

As noted above, though labels are organized into a tree hierarchy, 
there are actually several dimensions of commonality that suggest different groupings.
These attributes are encoded as ternary characteristics; 
for each label (including internal labels), every one of the 8 attributes  
is assigned a value of $+$, $-$, or $0$ (refer to \cref{fig:hierarchy}).
In order to capture these similarities in the model's features and cost function, 
we define the attribute vector function $\boldsymbol{\omega}(y) = $
\begin{equation*}
[y, A(y), B(y), G(y), O(y), P(y), R(y), S(y), U(y)]^{\top}
\end{equation*}
where $A: \mathcal{L} \cup \mathcal{I} \rightarrow \{+, -, 0\}$ returns the value for Anaphoric, 
$B(y)$ for Bridging, etc. The identity of the label is also included in the vector so that 
different labels are always recognized as different by the attribute function.
The categorical components of this vector are then binarized to form $\tilde{\boldsymbol{\omega}}(y)$; 
however, instead of a binary component that fires for the $0$ value of each ternary attribute, 
there is a component that fires for \emph{any} value of the attribute---a sort of bias term.
The weights assigned to features incorporating $+$ or $-$ attribute values, then, 
are easily interpreted as deviations relative to the bias.

\subsection{Cost and label space}\label{sec:cost}

The definiteness function hierarchy presented in \cref{fig:hierarchy} 
consists of 24~\emph{leaf labels}, which will be denoted $\mathcal{L}$, and 
10~more abstract \emph{intermediate labels}, denoted $\mathcal{I}$.
All of the gold labels in the training data are from $\mathcal{L}$, 
but we give our model the option to predict more abstract labels 
to receive partial credit.
We will therefore use $\mathcal{Y} = \mathcal{L} \cup \mathcal{I}$.

The relatedness of leaf label pairs is determined by the dot product of the two original attribute vectors: 
let $\Delta(\ell,\ell') = |\boldsymbol{\omega}(\ell) \cap \boldsymbol{\omega}(\ell')|^{-1}$.\footnote{By the intersection of two attribute vectors, we mean the subset of components that have a matching (categorical) value.} 
For a gold leaf label $\ell$ and an internal label $\iota$, 
$\Delta(\ell,\iota) =$ the distance in the hierarchy 
between $\ell$ and $\iota$.\nss{TODO: ensure if $\iota$ is an ancestor of $\ell$, this is less than choosing another leaf dominated by $\iota$}.
(There is no need to define $\Delta(\iota, \cdot)$, as the training set does not contain intermediate labels.)

\section{Evaluation}\label{sec:eval}

The following measures will be used to evaluate our predictor against the gold standard 
for the held-out evaluation (dev or test) set $\mathcal{E}$:
\begin{itemize}
  \item \textbf{Exact match:} This gives credit only where the predicted and gold labels 
  are identical. When the model is allowed to predict internal labels, we will report 
  overall precision and recall of leaf labels. Otherwise, we report accuracy.
  \item \textbf{By leaf label:} We also compute precision and recall of each leaf label 
  to determine which categories are reliably predicted.
  \item \textbf{Soft match:} This accuracy measure gives partial credit where the predicted and gold labels 
  are related. It is computed as the $\Delta$ function in \cref{sec:cost} \nss{normalized to be between 0 and 1?}.
  \item \textbf{Perplexity:} This determines how ``surprised'' our model is by the gold labels 
  in the test set; the greater the probability mass assigned to the true labels, 
  the higher the score. 
  It is computed as $2^{\left(\sum_{\langle x, y \rangle \in \mathcal{E}} \log_2 p_{\hat{\boldsymbol{\theta}}}(y | x)\right) / |\mathcal{E}|}$.
\end{itemize}

\section{Experiments}\label{sec:exp}

\nss{English: ±cost function, ±non-identity attributes, ±predicting intermediate labels}

\nss{maybe: which attribute groupings produce the best classifier, if we want to force a hierarchy}

\nss{feature/attribute ablations}

\nss{Hindi?}

\section{Conclusion}\label{sec:conclusion}





\bibliographystyle{aclnat}
% you bib file should really go here
\setlength{\bibsep}{10pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{definiteness}}


\end{document}


\section{Discussion: Error analysis, comparison across languages}\label{sec:discussion}

\ab{confusion matrix - Hindi vs Eng (for common annotations)- semantic annotations
differences in annotations- why- different grammatical constructions, e.g. NP in 1 language but predicate in another etc.
what do similarities tell us about grammaticalization, what do differences tell.
discuss- how this analysis can help in MT}

